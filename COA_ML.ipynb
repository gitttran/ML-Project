{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amy Perez\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#standard useful libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#model selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#neural network dependencies\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amy Perez\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (0,2,3,4,5,6,7,8,10,11,12,59,61,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WONUM</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Match_addr</th>\n",
       "      <th>Address</th>\n",
       "      <th>Comments</th>\n",
       "      <th>ARC_Address</th>\n",
       "      <th>InitiatedDateTime</th>\n",
       "      <th>StartDateTime</th>\n",
       "      <th>CompletedDateTime</th>\n",
       "      <th>InitiatedYr</th>\n",
       "      <th>...</th>\n",
       "      <th>PRESSURE</th>\n",
       "      <th>MAX_HEAD</th>\n",
       "      <th>MIN_HEAD</th>\n",
       "      <th>AVE_HEAD</th>\n",
       "      <th>MAX_PRESS</th>\n",
       "      <th>MIN_PRESS</th>\n",
       "      <th>AVE_PRESS</th>\n",
       "      <th>DIFF_PRESS</th>\n",
       "      <th>MUSYM</th>\n",
       "      <th>Failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>725328</td>\n",
       "      <td>7.545442</td>\n",
       "      <td>17 Cerro St SW, Atlanta, Georgia, 30314</td>\n",
       "      <td>Howell Mill Rd &amp; 17th St NW</td>\n",
       "      <td></td>\n",
       "      <td>HOWELL MILL RD &amp; 17TH ST NW</td>\n",
       "      <td>1/9/2014</td>\n",
       "      <td>1/9/2014</td>\n",
       "      <td>1/9/2014</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.1743</td>\n",
       "      <td>1199.7460</td>\n",
       "      <td>1178.2394</td>\n",
       "      <td>1191.5188</td>\n",
       "      <td>108.6975</td>\n",
       "      <td>99.3787</td>\n",
       "      <td>105.1327</td>\n",
       "      <td>9.3188</td>\n",
       "      <td>Ub</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>756318</td>\n",
       "      <td>2.714840</td>\n",
       "      <td>Dekalb Ave NE &amp; Elmira Pl NE, Atlanta, Georgia...</td>\n",
       "      <td>DeKalb Ave NE &amp; Elmira Pl NE</td>\n",
       "      <td>2ND WATER MAIN BREAK AT DEKALB AVE NE AND ELMI...</td>\n",
       "      <td>DeKalb Ave NE &amp; Elmira Pl NE</td>\n",
       "      <td>4/18/2014</td>\n",
       "      <td>4/20/2015</td>\n",
       "      <td>4/20/2015</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.7252</td>\n",
       "      <td>1197.4366</td>\n",
       "      <td>1176.4850</td>\n",
       "      <td>1187.7832</td>\n",
       "      <td>74.7168</td>\n",
       "      <td>65.6384</td>\n",
       "      <td>70.5340</td>\n",
       "      <td>9.0784</td>\n",
       "      <td>Ub</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1349957</td>\n",
       "      <td>8.354273</td>\n",
       "      <td>Evans Dr SW &amp; Astor Ave SW, Atlanta, Georgia, ...</td>\n",
       "      <td>Evans Dr SW &amp; Astor Ave SW</td>\n",
       "      <td>Call taken by: DROBERTSON</td>\n",
       "      <td>Evans Dr SW &amp; Astor Ave SW</td>\n",
       "      <td>11/2/2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>...</td>\n",
       "      <td>81.9982</td>\n",
       "      <td>1196.2609</td>\n",
       "      <td>1148.9755</td>\n",
       "      <td>1177.9572</td>\n",
       "      <td>97.6555</td>\n",
       "      <td>77.1668</td>\n",
       "      <td>89.7245</td>\n",
       "      <td>20.4888</td>\n",
       "      <td>Ub</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>780444</td>\n",
       "      <td>14.600671</td>\n",
       "      <td>Campbellton Fairburn Rd &amp; Ridge Rd, Fairburn, ...</td>\n",
       "      <td>CAMPBELLTON FAIRBURN RD &amp; RIDGE RD</td>\n",
       "      <td>PER JOHN SKINNER THIS IS A KNOCKED OFF HYDRANT...</td>\n",
       "      <td>CAMPBELLTON FAIRBURN RD &amp; RIDGE RD</td>\n",
       "      <td>7/6/2014</td>\n",
       "      <td>7/6/2014</td>\n",
       "      <td>7/6/2014</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>...</td>\n",
       "      <td>128.9117</td>\n",
       "      <td>1166.0968</td>\n",
       "      <td>1134.0135</td>\n",
       "      <td>1151.1066</td>\n",
       "      <td>136.5314</td>\n",
       "      <td>122.6298</td>\n",
       "      <td>130.0362</td>\n",
       "      <td>13.9017</td>\n",
       "      <td>AgB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>774585</td>\n",
       "      <td>15.977450</td>\n",
       "      <td>1 Hartsfield Center Pkwy, Atlanta, Georgia, 30354</td>\n",
       "      <td>1 HARTSFIELD CENTER PKWY SW</td>\n",
       "      <td></td>\n",
       "      <td>1 HARTSFIELD CENTER PKWY SW</td>\n",
       "      <td>6/6/2014</td>\n",
       "      <td>6/6/2014</td>\n",
       "      <td>6/6/2014</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Ub</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     WONUM   Distance                                         Match_addr  \\\n",
       "0   725328   7.545442            17 Cerro St SW, Atlanta, Georgia, 30314   \n",
       "1   756318   2.714840  Dekalb Ave NE & Elmira Pl NE, Atlanta, Georgia...   \n",
       "2  1349957   8.354273  Evans Dr SW & Astor Ave SW, Atlanta, Georgia, ...   \n",
       "3   780444  14.600671  Campbellton Fairburn Rd & Ridge Rd, Fairburn, ...   \n",
       "4   774585  15.977450  1 Hartsfield Center Pkwy, Atlanta, Georgia, 30354   \n",
       "\n",
       "                              Address  \\\n",
       "0         Howell Mill Rd & 17th St NW   \n",
       "1        DeKalb Ave NE & Elmira Pl NE   \n",
       "2          Evans Dr SW & Astor Ave SW   \n",
       "3  CAMPBELLTON FAIRBURN RD & RIDGE RD   \n",
       "4         1 HARTSFIELD CENTER PKWY SW   \n",
       "\n",
       "                                            Comments  \\\n",
       "0                                                      \n",
       "1  2ND WATER MAIN BREAK AT DEKALB AVE NE AND ELMI...   \n",
       "2                          Call taken by: DROBERTSON   \n",
       "3  PER JOHN SKINNER THIS IS A KNOCKED OFF HYDRANT...   \n",
       "4                                                      \n",
       "\n",
       "                          ARC_Address InitiatedDateTime StartDateTime  \\\n",
       "0         HOWELL MILL RD & 17TH ST NW          1/9/2014      1/9/2014   \n",
       "1        DeKalb Ave NE & Elmira Pl NE         4/18/2014     4/20/2015   \n",
       "2          Evans Dr SW & Astor Ave SW         11/2/2015           NaN   \n",
       "3  CAMPBELLTON FAIRBURN RD & RIDGE RD          7/6/2014      7/6/2014   \n",
       "4         1 HARTSFIELD CENTER PKWY SW          6/6/2014      6/6/2014   \n",
       "\n",
       "  CompletedDateTime  InitiatedYr   ...    PRESSURE   MAX_HEAD   MIN_HEAD  \\\n",
       "0          1/9/2014       2014.0   ...    102.1743  1199.7460  1178.2394   \n",
       "1         4/20/2015       2014.0   ...     69.7252  1197.4366  1176.4850   \n",
       "2               NaN       2015.0   ...     81.9982  1196.2609  1148.9755   \n",
       "3          7/6/2014       2014.0   ...    128.9117  1166.0968  1134.0135   \n",
       "4          6/6/2014       2014.0   ...      0.0000     0.0000     0.0000   \n",
       "\n",
       "    AVE_HEAD  MAX_PRESS  MIN_PRESS  AVE_PRESS DIFF_PRESS  MUSYM Failure  \n",
       "0  1191.5188   108.6975    99.3787   105.1327     9.3188     Ub       1  \n",
       "1  1187.7832    74.7168    65.6384    70.5340     9.0784     Ub       1  \n",
       "2  1177.9572    97.6555    77.1668    89.7245    20.4888     Ub       1  \n",
       "3  1151.1066   136.5314   122.6298   130.0362    13.9017    AgB       1  \n",
       "4     0.0000     0.0000     0.0000     0.0000     0.0000     Ub       1  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading data from csv\n",
    "pipes_df = pd.read_csv(\"wMainData 20190214.csv\")\n",
    "pipes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dummies for columns where applicable--MATERIAL, DESCRIPTION, PRESSURE_ZONE\n",
    "pipes_df = pd.concat([pipes_df, pd.get_dummies(pipes_df['ZONE'], prefix='P-Zone')], axis=1)\n",
    "pipes_df = pd.concat([pipes_df, pd.get_dummies(pipes_df['MATERIAL_12'], prefix='Material')], axis=1); \n",
    "\n",
    "#drop original columns--MATERIAL, DESCRIPTION, PRESSURE_ZONE\n",
    "pipes_df2 = pipes_df.drop(columns={ 'MATERIAL_12','ZONE'})\n",
    "\n",
    "#Delete unneeded columns\n",
    "#NOTE to Team: I didn’t see any data at all in the ‘DEMAND9’, ‘PATTERN9’, ‘DEMAND10’, ‘PATTERN10’ columns,so I deleted all of these columns\n",
    "pipes_df3 = pipes_df2.drop(columns={\"PATTERN1\", \"PATTERN2\", \"PATTERN3\", \"PATTERN4\", \"PATTERN5\", \"PATTERN6\", \"PATTERN7\", \n",
    "                                    \"PATTERN8\", \"PATTERN9\", \"PATTERN10\", \"DEMAND9\", \"DEMAND10\",'WONUM','Distance',\n",
    "                                    'Match_addr','Address','Comments','ARC_Address','StartDateTime',\n",
    "                                    'CompletedDateTime','DESCRIPTION','SrvcRqstTypDesc','MATERIAL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a column to be opposite of FAILURE column for use in neural network training\n",
    "tempSuccessList = []\n",
    "for row in np.arange(len(pipes_df3['MOID'])):\n",
    "    value = 1 - pipes_df3.iloc[row]['Failure']\n",
    "    tempSuccessList.append(value)\n",
    "pipes_df3['SUCCESS'] = tempSuccessList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InitiatedDateTime',\n",
       " 'InitiatedYr',\n",
       " 'INSTALLDATE_12',\n",
       " 'Year_Installed',\n",
       " 'AGE',\n",
       " 'DIAMETER_1',\n",
       " 'PLATCARD_12_13',\n",
       " 'DIAMETER_2',\n",
       " 'Length',\n",
       " 'DIAMETER_12_13',\n",
       " 'ROUGHNESS',\n",
       " 'MOID',\n",
       " 'STREETNAME',\n",
       " 'LINEYEAR',\n",
       " 'INDEX_DIAM',\n",
       " 'INDEX_YR',\n",
       " 'LENGTH',\n",
       " 'FLOW',\n",
       " 'VELOCITY',\n",
       " 'HEADLOSS',\n",
       " 'HL1000',\n",
       " 'MAX_FLOW',\n",
       " 'MIN_FLOW',\n",
       " 'AVE_FLOW',\n",
       " 'MAX_VELOC',\n",
       " 'MIN_VELOC',\n",
       " 'AVE_VELOC',\n",
       " 'MAX_HDLOSS',\n",
       " 'MIN_HDLOSS',\n",
       " 'AVE_HDLOSS',\n",
       " 'ELEVATION',\n",
       " 'DEMAND1',\n",
       " 'DEMAND2',\n",
       " 'DEMAND3',\n",
       " 'DEMAND4',\n",
       " 'DEMAND5',\n",
       " 'DEMAND6',\n",
       " 'DEMAND7',\n",
       " 'DEMAND8',\n",
       " 'RUN_ELEV',\n",
       " 'DEMAND',\n",
       " 'HEAD',\n",
       " 'PRESSURE',\n",
       " 'MAX_HEAD',\n",
       " 'MIN_HEAD',\n",
       " 'AVE_HEAD',\n",
       " 'MAX_PRESS',\n",
       " 'MIN_PRESS',\n",
       " 'AVE_PRESS',\n",
       " 'DIFF_PRESS',\n",
       " 'MUSYM',\n",
       " 'Failure',\n",
       " 'P-Zone_Chatt 1020',\n",
       " 'P-Zone_Hemp 1175',\n",
       " 'P-Zone_North 1225',\n",
       " 'P-Zone_WTP',\n",
       " 'Material_ ',\n",
       " 'Material_AC',\n",
       " 'Material_CAS',\n",
       " 'Material_COP',\n",
       " 'Material_DIP',\n",
       " 'Material_GP',\n",
       " 'Material_PVC',\n",
       " 'Material_RCP',\n",
       " 'Material_SP',\n",
       " 'Material_UNK',\n",
       " 'Material_W.I.',\n",
       " 'SUCCESS']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipes_df3.head()\n",
    "pipes_df3.columns.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select which columns to train off of.\n",
    "#A number of iterations of this were initially used when exploring the data\n",
    "#In the end it was decided to only use average values for those which have averages\n",
    "\n",
    "inputColumns =['DIAMETER_2','ROUGHNESS','AGE','LENGTH','FLOW','VELOCITY','HEADLOSS','HL1000','MAX_FLOW','MIN_FLOW',\n",
    "               'AVE_FLOW','MAX_VELOC','MIN_VELOC','AVE_VELOC','MAX_HDLOSS','MIN_HDLOSS','AVE_HDLOSS','DEMAND','MAX_HEAD',\n",
    "               'MIN_HEAD','AVE_HEAD','MAX_PRESS','MIN_PRESS','AVE_PRESS','DIFF_PRESS','P-Zone_Chatt 1020',\n",
    "               'P-Zone_Hemp 1175','P-Zone_North 1225','P-Zone_WTP','Material_AC','Material_CAS','Material_COP',\n",
    "               'Material_DIP','Material_GP','Material_PVC','Material_RCP','Material_SP','Material_UNK','Material_W.I.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(pipes_df3[inputColumns],pipes_df3['Failure'],stratify=pipes_df3['Failure'],test_size = 0.3, random_state=42)\n",
    "\n",
    "#Make another split for y which uses the additional 'SUCCESS' column for use in neural network\n",
    "#Note: Using random state the X_train and X_test would be identical and so need not be kept\n",
    "_, _, y_train_2, y_test_2 = train_test_split(pipes_df3[inputColumns],pipes_df3[['Failure','SUCCESS']],stratify=pipes_df3['Failure'],test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: If we were to train and test on the split as it currently is, it will just guess that everything is an unbroken pipe\n",
    "#This is not desired behavior\n",
    "\n",
    "#Split X_train into a list of those where the pipes are broken and those which arent\n",
    "#Then, undersample those pipes which are unbroken to avoid bias\n",
    "X_train_false = X_train[y_train==0].sample(n=sum(y_train), random_state=42)\n",
    "X_train_true = X_train[y_train==1].sample(n=sum(y_train), random_state=42)\n",
    "\n",
    "#Recombine the sampled X_train into one list and keep only those y_train values corresponding to selected indices\n",
    "X_train_sampled = X_train_true.append(X_train_false)\n",
    "y_train_sampled = y_train.loc[X_train_sampled.index]\n",
    "y_train_sampled_2 = y_train_2.loc[X_train_sampled.index]\n",
    "\n",
    "#Apply min-max scalers to X\n",
    "#Note: applying min-max too early changed format of dataframe into a np.array and interfered with the sampling process\n",
    "X_minmax = MinMaxScaler().fit(X_train)\n",
    "X_train_minmax = X_minmax.transform(X_train_sampled)\n",
    "X_test_minmax = X_minmax.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7532356948228883"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set a logistic regression as a model\n",
    "lrmodel = LogisticRegression()\n",
    "lrmodel.fit(X_train_minmax,y_train_sampled)\n",
    "lrmodel.score(X_train_minmax,y_train_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7309427217793413"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrmodel.score(X_test_minmax,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23837,  8896],\n",
       "       [  588,  1928]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,lrmodel.predict(X_test_minmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 1s - loss: 0.6586 - acc: 0.6074\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.6410 - acc: 0.6277\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.6306 - acc: 0.6413\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.6224 - acc: 0.6489\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.6120 - acc: 0.6619\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.6008 - acc: 0.6790\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.5971 - acc: 0.6786\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.5882 - acc: 0.6899\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.5823 - acc: 0.6960\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.5776 - acc: 0.7018\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.5699 - acc: 0.7127\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.5654 - acc: 0.7194\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.5616 - acc: 0.7131\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.5571 - acc: 0.7245\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.5570 - acc: 0.7240\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.5559 - acc: 0.7255\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.5500 - acc: 0.7265\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.5512 - acc: 0.7263\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.5493 - acc: 0.7240\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.5450 - acc: 0.7307\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.5456 - acc: 0.7255\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.5399 - acc: 0.7330\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.5358 - acc: 0.7399\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.5368 - acc: 0.7349\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.5353 - acc: 0.7368\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.5290 - acc: 0.7425\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.5279 - acc: 0.7425\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.5237 - acc: 0.7498\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.5230 - acc: 0.7504\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.5227 - acc: 0.7504\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.5164 - acc: 0.7517\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.5180 - acc: 0.7527\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.5159 - acc: 0.7506\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.5128 - acc: 0.7515\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.5081 - acc: 0.7552\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.5067 - acc: 0.7586\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.5012 - acc: 0.7640\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.5026 - acc: 0.7649\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.5002 - acc: 0.7653\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.4992 - acc: 0.7624\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.4968 - acc: 0.7678\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.4921 - acc: 0.7711\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.4911 - acc: 0.7720\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.4920 - acc: 0.7695\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.4864 - acc: 0.7743\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.4848 - acc: 0.7718\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.4822 - acc: 0.7745\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.4783 - acc: 0.7816\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.4828 - acc: 0.7758\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.4747 - acc: 0.7856\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.4773 - acc: 0.7797\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.4706 - acc: 0.7835\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.4691 - acc: 0.7827\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.4684 - acc: 0.7852\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.4653 - acc: 0.7875\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.4690 - acc: 0.7841\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.4622 - acc: 0.7883\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.4623 - acc: 0.7915\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.4593 - acc: 0.7866\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.4603 - acc: 0.7917\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.4575 - acc: 0.7877\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.4571 - acc: 0.7887\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.4570 - acc: 0.7902\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.4546 - acc: 0.7902\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.4526 - acc: 0.7946\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.4515 - acc: 0.7950\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.4515 - acc: 0.7938\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.4472 - acc: 0.7959\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.4460 - acc: 0.7975\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.4464 - acc: 0.7946\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.4430 - acc: 0.7933\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.4445 - acc: 0.7938\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.4428 - acc: 0.7948\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.4451 - acc: 0.7988\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.4388 - acc: 0.7969\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.4416 - acc: 0.7990\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.4397 - acc: 0.7998\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.4400 - acc: 0.7984\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.4423 - acc: 0.7952\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.4411 - acc: 0.7942\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.4351 - acc: 0.8063\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.4399 - acc: 0.7982\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.4353 - acc: 0.7940\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.4386 - acc: 0.7990\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.4312 - acc: 0.8044\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.4319 - acc: 0.7988\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.4344 - acc: 0.7959\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.4318 - acc: 0.8038\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.4314 - acc: 0.7994\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.4302 - acc: 0.8000\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.4295 - acc: 0.8023\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.4266 - acc: 0.8015\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.4271 - acc: 0.8034\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.4301 - acc: 0.7982\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.4297 - acc: 0.8053\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.4289 - acc: 0.8017\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.4244 - acc: 0.8053\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.4257 - acc: 0.8095\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.4258 - acc: 0.8030\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.4277 - acc: 0.8026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26612ac0630>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add a shallow neural network\n",
    "nnmodel = Sequential()\n",
    "\n",
    "#Add layers\n",
    "nnmodel.add(Dense(100, activation='relu', input_dim=X_train_minmax.shape[1]))\n",
    "nnmodel.add(Dense(2,activation = 'softmax'))\n",
    "\n",
    "#Compile\n",
    "nnmodel.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "#Fit the training model to the data\n",
    "nnmodel.fit(\n",
    "    X_train_minmax,\n",
    "    y_train_sampled_2,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4486992735858192, Accuracy: 0.8023089740537968\n"
     ]
    }
   ],
   "source": [
    "nnmodel_loss, nnmodel_accuracy = nnmodel.evaluate(X_test_minmax, y_test_2, verbose=2)\n",
    "print(f\"Loss: {nnmodel_loss}, Accuracy: {nnmodel_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide on number of estimators to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] n_estimators=20 .................................................\n",
      "[CV] ........ n_estimators=20, score=0.8455056179775281, total=   1.0s\n",
      "[CV] n_estimators=20 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......... n_estimators=20, score=0.845426673479816, total=   1.0s\n",
      "[CV] n_estimators=20 .................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    2.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........ n_estimators=20, score=0.8390393459376597, total=   1.4s\n",
      "[CV] n_estimators=50 .................................................\n",
      "[CV] ........ n_estimators=50, score=0.8503575076608785, total=   2.8s\n",
      "[CV] n_estimators=50 .................................................\n",
      "[CV] ........ n_estimators=50, score=0.8546244251405212, total=   2.2s\n",
      "[CV] n_estimators=50 .................................................\n",
      "[CV] ........ n_estimators=50, score=0.8530914665304037, total=   2.6s\n",
      "[CV] n_estimators=100 ................................................\n",
      "[CV] ....... n_estimators=100, score=0.8549540347293156, total=   5.4s\n",
      "[CV] n_estimators=100 ................................................\n",
      "[CV] ....... n_estimators=100, score=0.8571793561573837, total=   5.5s\n",
      "[CV] n_estimators=100 ................................................\n",
      "[CV] ....... n_estimators=100, score=0.8515585079202862, total=   4.5s\n",
      "[CV] n_estimators=200 ................................................\n",
      "[CV] ....... n_estimators=200, score=0.8526557711950971, total=   8.7s\n",
      "[CV] n_estimators=200 ................................................\n",
      "[CV] ....... n_estimators=200, score=0.8587123147675013, total=  11.2s\n",
      "[CV] n_estimators=200 ................................................\n",
      "[CV] ....... n_estimators=200, score=0.8515585079202862, total=   9.8s\n",
      "[CV] n_estimators=300 ................................................\n",
      "[CV] ........ n_estimators=300, score=0.855464759959142, total=  18.0s\n",
      "[CV] n_estimators=300 ................................................\n",
      "[CV] ....... n_estimators=300, score=0.8579458354624425, total=  16.3s\n",
      "[CV] n_estimators=300 ................................................\n",
      "[CV] ......... n_estimators=300, score=0.85334695963209, total=  14.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [20, 50, 100, 200, 300]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load a Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "#Set a parameter grid\n",
    "#Only real parameters we needed to check was the number of estimators\n",
    "param_grid = {'n_estimators': [20,50,100,200,300]}\n",
    "grid = GridSearchCV(model, param_grid, verbose=3)\n",
    "\n",
    "#Fit the grid\n",
    "grid.fit(X_train_minmax,y_train_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score was 0.8357400209935033\n",
      "The best parameters were {'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "#Find the best score and best parameters\n",
    "print(f'The best score was {grid.score(X_test_minmax,y_test)}\\nThe best parameters were {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.83      0.90     32733\n",
      "          1       0.29      0.90      0.44      2516\n",
      "\n",
      "avg / total       0.94      0.84      0.87     35249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Take a look at classification report to confirm that the model is ideal\n",
    "print(classification_report(y_test, grid.predict(X_test_minmax)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27193,  5540],\n",
       "       [  250,  2266]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look at the confusion matrix to confirm that there is no unintended behavior (e.g. only guessing unbroken)\n",
    "confusion_matrix(y_test, grid.predict(X_test_minmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing deeper analysis with Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply cross validation so as to have percentage predictions for all data points without bias towards training data\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "split = cv.split(pipes_df3[inputColumns],pipes_df3['Failure'])\n",
    "rfmodel = RandomForestClassifier(n_estimators=300, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new column for use in displaying results of analysis to be filled in later\n",
    "#Final results will be stored in final_df\n",
    "scores = []\n",
    "final_df = pipes_df3\n",
    "final_df['BurstChance'] = 0\n",
    "confusion = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18342  3480]\n",
      " [  142  1536]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amy Perez\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18565  3256]\n",
      " [  172  1506]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amy Perez\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18632  3189]\n",
      " [  171  1507]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amy Perez\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18291  3530]\n",
      " [  163  1514]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amy Perez\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18441  3380]\n",
      " [  182  1495]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amy Perez\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#For each train-test split from our crossvalidation, run the same type of analysis as before\n",
    "for train_idx, test_idx in cv.split(pipes_df3[inputColumns],pipes_df3['Failure']):\n",
    "    Current_X_train = pipes_df3[inputColumns].loc[train_idx]\n",
    "    Current_X_test = pipes_df3[inputColumns].loc[test_idx]\n",
    "    Current_y_train = pipes_df3['Failure'].loc[train_idx]\n",
    "    Current_y_test = pipes_df3['Failure'].loc[test_idx]\n",
    "    \n",
    "    \n",
    "    #Undersample training data as before\n",
    "    Current_X_train_false = Current_X_train[Current_y_train==0].sample(n=sum(Current_y_train))\n",
    "    Current_X_train_true = Current_X_train[Current_y_train==1].sample(n=sum(Current_y_train))\n",
    "    \n",
    "    Current_X_train_sampled = Current_X_train_true.append(Current_X_train_false)\n",
    "    Current_y_train_sampled = Current_y_train.loc[Current_X_train_sampled.index]\n",
    "    \n",
    "    Current_X_minmax = MinMaxScaler().fit(Current_X_train)\n",
    "    Current_X_train_minmax = Current_X_minmax.transform(Current_X_train_sampled)\n",
    "    Current_X_test_minmax = Current_X_minmax.transform(Current_X_test)\n",
    "\n",
    "    \n",
    "    rfmodel.fit(Current_X_train_minmax, Current_y_train_sampled)\n",
    "    score = rfmodel.score(Current_X_test_minmax, Current_y_test)\n",
    "    scores.append(score)\n",
    "    currentconfusion = confusion_matrix(Current_y_test,rfmodel.predict(Current_X_test_minmax))\n",
    "    print(currentconfusion)\n",
    "    confusion.append([currentconfusion[0][0],currentconfusion[0][1],currentconfusion[1][0],currentconfusion[1][1]])\n",
    "    \n",
    "    #Store prediction probabilities into final_df\n",
    "    final_df['BurstChance'].loc[test_idx] = [x[1] for x in list(rfmodel.predict_proba(Current_X_test_minmax))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at combined confusion matrix\n",
    "truepositive = confusion[0][0] + confusion[1][0]+confusion[2][0]+confusion[3][0]+confusion[4][0]\n",
    "falsenegative = confusion[0][1] + confusion[1][1]+confusion[2][1]+confusion[3][1]+confusion[4][1]\n",
    "falsepositive = confusion[0][2] + confusion[1][2]+confusion[2][2]+confusion[3][2]+confusion[4][2]\n",
    "truenegative = confusion[0][3] + confusion[1][3]+confusion[2][3]+confusion[3][3]+confusion[4][3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[92271, 16835]\n",
      "[830, 7558]]\n"
     ]
    }
   ],
   "source": [
    "print(f'[{[truepositive, falsenegative]}\\n{[falsepositive,truenegative]}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8458723404255319,\n",
       " 0.8541214519766799,\n",
       " 0.8570151921358355,\n",
       " 0.842837688313899,\n",
       " 0.8484126308622011]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at final scores for each\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BurstChance</th>\n",
       "      <th>MOID</th>\n",
       "      <th>DIAMETER_2</th>\n",
       "      <th>Length</th>\n",
       "      <th>ROUGHNESS</th>\n",
       "      <th>AGE</th>\n",
       "      <th>LENGTH</th>\n",
       "      <th>FLOW</th>\n",
       "      <th>VELOCITY</th>\n",
       "      <th>HEADLOSS</th>\n",
       "      <th>...</th>\n",
       "      <th>Material_AC</th>\n",
       "      <th>Material_CAS</th>\n",
       "      <th>Material_COP</th>\n",
       "      <th>Material_DIP</th>\n",
       "      <th>Material_GP</th>\n",
       "      <th>Material_PVC</th>\n",
       "      <th>Material_RCP</th>\n",
       "      <th>Material_SP</th>\n",
       "      <th>Material_UNK</th>\n",
       "      <th>Material_W.I.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14811</th>\n",
       "      <td>1.0</td>\n",
       "      <td>P52661</td>\n",
       "      <td>2.0</td>\n",
       "      <td>194.184290</td>\n",
       "      <td>30</td>\n",
       "      <td>78</td>\n",
       "      <td>194.184290</td>\n",
       "      <td>1.1791</td>\n",
       "      <td>0.1204</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95981</th>\n",
       "      <td>1.0</td>\n",
       "      <td>P148456</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.058445</td>\n",
       "      <td>121</td>\n",
       "      <td>53</td>\n",
       "      <td>9.058445</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11502</th>\n",
       "      <td>1.0</td>\n",
       "      <td>P35890</td>\n",
       "      <td>2.0</td>\n",
       "      <td>496.622452</td>\n",
       "      <td>40</td>\n",
       "      <td>42</td>\n",
       "      <td>496.622452</td>\n",
       "      <td>0.6176</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93480</th>\n",
       "      <td>1.0</td>\n",
       "      <td>P50595</td>\n",
       "      <td>2.0</td>\n",
       "      <td>424.786041</td>\n",
       "      <td>30</td>\n",
       "      <td>58</td>\n",
       "      <td>424.786041</td>\n",
       "      <td>0.7623</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>0.1688</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97912</th>\n",
       "      <td>1.0</td>\n",
       "      <td>P174490</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.081598</td>\n",
       "      <td>121</td>\n",
       "      <td>55</td>\n",
       "      <td>7.081598</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       BurstChance     MOID  DIAMETER_2      Length  ROUGHNESS  AGE  \\\n",
       "14811          1.0   P52661         2.0  194.184290         30   78   \n",
       "95981          1.0  P148456         6.0    9.058445        121   53   \n",
       "11502          1.0   P35890         2.0  496.622452         40   42   \n",
       "93480          1.0   P50595         2.0  424.786041         30   58   \n",
       "97912          1.0  P174490         6.0    7.081598        121   55   \n",
       "\n",
       "           LENGTH    FLOW  VELOCITY  HEADLOSS      ...        Material_AC  \\\n",
       "14811  194.184290  1.1791    0.1204    0.1732      ...                  0   \n",
       "95981    9.058445  0.0000    0.0000    0.0000      ...                  0   \n",
       "11502  496.622452  0.6176    0.0631    0.0785      ...                  0   \n",
       "93480  424.786041  0.7623    0.0779    0.1688      ...                  0   \n",
       "97912    7.081598  0.0000    0.0000    0.0000      ...                  0   \n",
       "\n",
       "       Material_CAS  Material_COP  Material_DIP  Material_GP  Material_PVC  \\\n",
       "14811             0             0             0            0             0   \n",
       "95981             1             0             0            0             0   \n",
       "11502             0             0             0            0             0   \n",
       "93480             0             0             0            0             0   \n",
       "97912             1             0             0            0             0   \n",
       "\n",
       "       Material_RCP  Material_SP  Material_UNK  Material_W.I.  \n",
       "14811             0            0             0              1  \n",
       "95981             0            0             0              0  \n",
       "11502             0            0             0              1  \n",
       "93480             0            0             0              1  \n",
       "97912             0            0             0              0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Output top ten most likely pipes to burst which are currently unbroken according to our model\n",
    "out = final_df[['BurstChance','MOID'] + inputColumns].loc[final_df['Failure']==0].sort_values('BurstChance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.2334, 'AGE'), (0.0821, 'Material_DIP'), (0.0491, 'ROUGHNESS'), (0.0432, 'LENGTH'), (0.0408, 'Length'), (0.0364, 'Material_CAS'), (0.0336, 'MAX_HEAD'), (0.0326, 'DIAMETER_2'), (0.0322, 'MIN_HEAD'), (0.0319, 'AVE_HEAD'), (0.0305, 'DEMAND'), (0.0284, 'DIFF_PRESS'), (0.0247, 'MIN_PRESS'), (0.0241, 'MAX_PRESS'), (0.0237, 'AVE_PRESS'), (0.0209, 'AVE_FLOW'), (0.0208, 'MAX_FLOW'), (0.0207, 'FLOW'), (0.0198, 'MIN_FLOW'), (0.0191, 'MAX_VELOC'), (0.0187, 'MIN_VELOC'), (0.0186, 'AVE_VELOC'), (0.0182, 'VELOCITY'), (0.0173, 'AVE_HDLOSS'), (0.0167, 'MAX_HDLOSS'), (0.016, 'HEADLOSS'), (0.0159, 'MIN_HDLOSS'), (0.0115, 'HL1000'), (0.0049, 'Material_W.I.'), (0.0045, 'Material_UNK'), (0.0038, 'P-Zone_North 1225'), (0.0038, 'P-Zone_Hemp 1175'), (0.0013, 'P-Zone_Chatt 1020'), (0.0003, 'Material_SP'), (0.0002, 'Material_COP'), (0.0001, 'P-Zone_WTP'), (0.0001, 'Material_PVC'), (0.0001, 'Material_GP'), (0.0, 'Material_RCP'), (0.0, 'Material_AC')]\n"
     ]
    }
   ],
   "source": [
    "#Output most relevant parameters according to our recent model\n",
    "print(sorted(zip(map(lambda x: round(x, 4), rfmodel.feature_importances_), inputColumns), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92     32733\n",
      "          1       0.33      0.98      0.49      2516\n",
      "\n",
      "avg / total       0.95      0.86      0.89     35249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,rfmodel.predict(X_test_minmax)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv('output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
